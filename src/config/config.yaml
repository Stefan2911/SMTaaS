logging-level: 10 # logging.DEBUG

smt:
  solver-location: "/usr/bin/cvc4"
  final-node: False # if True -> no further decision-making
  decision-mode: q_learning
  # decision-mode: deep_q_network

ev3:
  in-use: False
  smt:
    watch-directory: /home/robot/watch/  # in this directory the SMT problems are added

evaluation:
  cloud-instances:
    - http://194.182.171.9:5000/formulae  # Ubuntu VM at Frankfurt
    - http://10.10.20.1:5000/formulae     # 8 core VM at TU Wien (128.131.57.123)
    - http://10.10.20.2:5000/formulae     # 4 core VM at TU Wien (128.131.57.122)
    - http://10.10.20.3:5000/formulae     # 2 core VM at TU Wien (128.131.57.103)
  ded-instances:
    - http://10.0.0.19:5000/formulae
    - http://10.0.0.16:5000/formulae

decision:
  reinforcement-learning:
    reward-modes:
      time-aware:
        active: True
        ranges: # start inclusive, # end exclusive, currently dynamic mode is used
      traffic-aware:
        active: False

    training-smt-problem-directory-edge: src/smt/sets/training_edge
    training-smt-problem-directory-robot: src/smt/sets/training_robot

    solver:
      native: True
      instances:
        edge:
          - http://10.0.0.19:5000/formulae      # RPi
          - http://10.0.0.16:5000/formulae      # RPi
        cloud:
          - http://194.182.171.9:5000/formulae  # Ubuntu VM at Frankfurt
          - http://10.10.20.1:5000/formulae     # 8 core VM at TU Wien (128.131.57.123)
          - http://10.10.20.2:5000/formulae     # 4 core VM at TU Wien (128.131.57.122)
          - http://10.10.20.3:5000/formulae     # 2 core VM at TU Wien (128.131.57.103)
    common-hyper-parameters:
      gamma: 0.5          # discount factor used in the Bellman equation
      eps-start: 0.9      # starting value of epsilon (epsilon = exploration rate)
      eps-end: 0.1        # ending value of epsilon
      eps-decay: 0.01     # decay rate used to decay epsilon over time
      lr: 0.5             # learning rate (alpha)
      num-episodes: 40    # number of episodes

    q-learning:
      q-table-location: src/decision/reinforcement_learning/q_learning/q_table.npy

    deep-q-network:
      hyper-parameters:
        batch-size: 32
        target-update: 2  # defines how frequently (in terms of episodes), the target network weights are updated
        memory-size: 1000 # capacity of replay memory

      neural-network-location: src/decision/reinforcement_learning/deep_q_network/neural_network

uplink-cost: 1 # costs per KB
invocation-cost: 10 # costs per invocation

monitoring:

  update-period: 5 # in seconds

  connectivity:
    hosts:
      edge:
        - 10.0.0.19     # RPi
        - 10.0.0.16     # RPi
      cloud:
        - 194.182.171.9 # Ubuntu VM at Frankfurt
        - 10.10.20.1    # 8 core VM at TU Wien (128.131.57.123)
        - 10.10.20.2    # 4 core VM at TU Wien (128.131.57.122)
        - 10.10.20.3    # 2 core VM at TU Wien (128.131.57.103)

  indicators:
    connectivity: # in ms
      excellent:
        start: 0
        end: 50
      average:
        start: 51
        end: 150
      poor:
        start: 151
        end: 500
    cpu-usage: # in %
      excellent:
        start: 0
        end: 35
      average:
        start: 36
        end: 75
      poor:
        start: 76
        end: 100
    memory-usage: # in %
      excellent:
        start: 0
        end: 35
      average:
        start: 36
        end: 75
      poor:
        start: 76
        end: 100
    offload-cost:
      excellent:
        start: 0
        end: 35
      average:
        start: 36
        end: 75
      poor:
        start: 76
        end: 100
    problem-complexity:
      excellent: # easy
        start: 0
        end: 35
      average: # medium
        start: 36
        end: 75
      poor: # hard
        start: 76
        end: 100

  simulation:
    active: False
