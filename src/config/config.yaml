logging-level: 20 # logging.DEBUG

smt:
  solver-location: "/usr/bin/cvc4"
  # decision-mode: deep_q_network
  # decision-mode: none

ev3:
  in-use: False
  smt:
    watch-directory: /home/robot/watch/  # in this directory the SMT problems are added

evaluation:
  cloud-instances:
    - http://194.182.171.9:5000/formulae  # Ubuntu VM at Frankfurt
    - http://10.10.20.1:5000/formulae     # 8 core VM at TU Wien (128.131.57.123)
    - http://10.10.20.2:5000/formulae     # 4 core VM at TU Wien (128.131.57.122)
    - http://10.10.20.3:5000/formulae     # 2 core VM at TU Wien (128.131.57.103)
  ded-instances:
    - http://10.0.0.1:5000/formulae
    - http://10.0.0.3:5000/formulae

decision:
  reinforcement-learning:
    reward-modes:
      time-aware:
        active: True
        ranges: # start inclusive, # end exclusive, currently dynamic mode is used
      traffic-aware:
        active: False

    training-smt-problem-directory-edge: src/smt/sets/training_edge
    training-smt-problem-directory-robot: src/smt/sets/training_robot

    solver:
      native: True
      instances:
        edge:
          - http://10.0.0.1:5000/formulae      # RPi
          - http://10.0.0.3:5000/formulae      # RPi
        cloud:
          - http://194.182.171.9:5000/formulae  # Ubuntu VM at Frankfurt
          - http://10.10.20.1:5000/formulae     # 8 core VM at TU Wien (128.131.57.123)
          - http://10.10.20.2:5000/formulae     # 4 core VM at TU Wien (128.131.57.122)
          - http://10.10.20.3:5000/formulae     # 2 core VM at TU Wien (128.131.57.103)
    common-hyper-parameters:
      gamma: 0            # discount factor used in the Bellman equation (we only value immediate rewards)
      eps-start: 1        # starting value of epsilon (epsilon = exploration rate) (for training we want always explore)
      eps-end: 1          # ending value of epsilon (for training, we always want exploring)
      eps-decay: 0.001    # decay rate used to decay epsilon over time
      lr: 0.001           # learning rate (alpha) (learning rate for DQN is set by Adam optimizer)
      num-episodes: 40    # number of episodes

    q-learning:
      q-table-location: src/decision/reinforcement_learning/q_learning/q_table.npy

    deep-q-network:
      hyper-parameters:
        batch-size: 32
        target-update: 32 # defines how frequently (in terms of episodes), the target network weights are updated
        memory-size: 1000 # capacity of replay memory
      neural-network-location: src/decision/reinforcement_learning/deep_q_network/neural_network

uplink-cost: 1 # costs per KB
invocation-cost: 10 # costs per invocation

monitoring:
  update-period: 5 # in seconds
  is-training-active: False
  connectivity:
    hosts:
      edge:
        - 10.0.0.1     # RPi
        - 10.0.0.3     # RPi
      cloud:
        - 194.182.171.9 # Ubuntu VM at Frankfurt
        - 10.10.20.1    # 8 core VM at TU Wien (128.131.57.123)
        - 10.10.20.2    # 4 core VM at TU Wien (128.131.57.122)
        - 10.10.20.3    # 2 core VM at TU Wien (128.131.57.103)
    max-rtt: 450
  max-problem-complexity: 120

simulation:
  latencies:
    - 0
    - 100
    - 200
    - 300
    - 400
